<!DOCTYPE html>
<html>
	<head>
	<meta name="generator" content="Hugo 0.57.2" />
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Weijie Su&nbsp;- USTC</title><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
	<link rel="manifest" href="site.webmanifest">
	<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
	<meta name="msapplication-TileColor" content="#da532c">
	<meta name="theme-color" content="#ffffff">

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="alternate" type="application/rss+xml" href="https://jackroos.github.io/index.xml" title="Weijie Su" />
	<meta property="og:title" content="Weijie Su" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://jackroos.github.io/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Weijie Su"/>
<meta name="twitter:description" content=""/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="css/main.css" /><script src="js/feather.min.js"></script><script src="js/main.js"></script>
</head>


	<body>
		<div class="container wrapper">
			<div class="header">
	<img src=https://jackroos.github.io/20250503_083012422_iOS_square.jpg class="profile_image">
	<h1 class="site-title">Weijie Su</h1>
	<h2>(苏伟杰)</h2>
	<div class="site-affilation">
		<span class="affilation"><ul class="flat">
				<li class="position">Researcher, Shanghai AI Laboratory</li>
				<li class="email">jackroos@mail.ustc.edu.cn</li>
			</ul></span>
		

		<nav class="nav social">
			<ul class="flat"><a href="https://github.com/jackroos" title="GitHub"><i data-feather="github"></i></a><a href="https://twitter.com/jackroos237" title="Twitter"><i data-feather="twitter"></i></a><a href="https://www.linkedin.com/in/weijie-su-abb163177/" title="LinkedIn"><i data-feather="linkedin"></i></a>
				|&nbsp;<a href="https://scholar.google.com/citations?user=ECDe6IIAAAAJ&amp;hl=en" class='blog_cv'>[Google Scholar]</a>
				
			</ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
		</ul>
	</nav>
</div>

			
			<div class="introduction">
	<h4><b>Short Bio</b></h4><span>
		<p>Currently, Weijie Su is a Researcher at Shanghai AI Laboratory, headed by Prof. <a href="https://jifengdai.org/">Jifeng Dai</a>. He also works closely with Dr. <a href="https://scholar.google.com/citations?user=02RXI00AAAAJ">Xizhou Zhu</a> and Dr. <a href="https://whai362.github.io/">Wenhai Wang</a>. </p>
		<p>Weijie Su received Ph.D. degree from University of Science and Technology of China (USTC) in June 2024, under the supervision of Prof. <a href="http://staff.ustc.edu.cn/~binli/">Bin Li</a> and Prof. <a href="https://jifengdai.org/">Jifeng Dai</a>. Before that, he received his B.S. degree from USTC in June 2019, majoring in Information and Computing Science (Computational Mathematics). </p>
	</span>
	<br><span>
		<b>Research Interests&nbsp;</b> Multi-modal Foundation Models; General Capable Agents; Visual Foundation Models; Visual Recognition.
	</span>
</div>

                        <div class="introduction">
	<span>
		<p><b>If you are interested in an internship or joint Ph.D. program at Shanghai AI Laboratory related to my research field, please send me an email.</b></p>
	</span>
</div>
			
			<div class="introduction">
	<h4><b><i>News</i></b></h4><span>
		<strong><em>2025.05 &nbsp; &nbsp; Introducing <a href="https://arxiv.org/abs/2505.23762">ZeroGUI</a>, automating online GUI learning at zero human cost. </em></strong>
	</span><span>
        <em>2025.04 &nbsp; &nbsp; Our InternVL3 is released. </em>
	</span><span>
		<em>2025.02 &nbsp; &nbsp; Our paper <a href="https://github.com/OpenGVLab/PVC">PVC</a> got accepted by CVPR 2025. </em>
	</span><span>
		<em>2024.09 &nbsp; &nbsp; One paper got accepted by NeurIPS 2024. </em>
	</span><span>
		<em>2023.05 &nbsp; &nbsp; Introducing <a href="https://github.com/OpenGVLab/GITM">GITM</a>, a generally capable agent that fully masters Minecraft. </em>
	</span><span>
		<em>2023.02 &nbsp; &nbsp; Our M3I Pre-training and SiameseIM are accepted by CVPR 2023. </em>
	</span><span>
		<em>2021.06 &nbsp; &nbsp; Presentation slides and video of our Deformable DETR on ICLR 2021 are available now. </em>
	</span><span>
		<em>2021.01 &nbsp; &nbsp; Deformable DETR is accepted for oral presentation by ICLR 2021</a>.</em>
	</span><span>
	</span><span>
		<em>2020.11 &nbsp; &nbsp; Code of Deformable DETR is now available at <a href="https://github.com/fundamentalvision/Deformable-DETR">this github repository</a>.</em>
	</span><span>
		<em>2020.05 &nbsp; &nbsp; Presentation slides and video of our VL-BERT on ICLR 2020 are available now.</em>
	</span><span>
		<em>2019.12 &nbsp; &nbsp; Our paper <a href="https://arxiv.org/abs/1908.08530">VL-BERT</a> got accepted by ICLR 2020.</em>
	</span><span>
		<em>2019.11 &nbsp; &nbsp; Code for our paper <a href="https://arxiv.org/abs/1908.08530">VL-BERT</a> has been made public available at <a href="https://github.com/jackroos/VL-BERT">github</a>.</em>
	</span></div>

			

			<div class="introduction">
	<h4><b>Experiences</b></h4><ul>
		<li>
			<em>2024.1 - Present</em>&nbsp; &nbsp;Researcher, Shanghai AI Laboratory.
		</li>
	</ul><ul>
		<li>
			<em>2020.1 - 2024.1</em>&nbsp; &nbsp;Research Intern, SenseTime Research.
		</li>
	</ul><ul>
		<li>
			<em>2018.7 - 2019.9</em>&nbsp; &nbsp; Research Intern, Visual Computing Group, Microsoft Research Asia.
		</li>
	</ul><ul>
		<li>
			<em>2017.7 - 2018.6</em>&nbsp; &nbsp; Research Intern, <a href="http://gcl.ustc.edu.cn/">GCL Lab</a> at USTC.
		</li>
	</ul></div>
			<div class="introduction">
	<h4><b>Honors</b></h4><ul>
		<li>
			Outstanding Graduate, University of Science and Technology of China, 2024.
		</li>
	</ul><ul>
	        <li>
			Future Star Award (未来之星奖), SenseTime, 2024.
		</li>
	</ul><ul>
		<li>
			National Scholarship (国家奖学金), Ministry of Education of the People's Republic of China, 2020.
		</li>
	</ul><ul>
		<li>
			Outstanding Graduate, University of Science and Technology of China, 2019.
		</li>
	</ul><ul>
		<li>
			Bronze Medal, The 30th Chinese Mathematical Olympiad (CMO), 2014.
		</li>
	</ul></div>
			<div class="introduction">
	
</div>

			<div class="introduction">

	<h4><b>Publications</b></h4>
    <h6>(<sup>*</sup> Equal Contribution; <sup>†</sup> Project Lead; <sup><i class="far fa-envelope"></i></sup> Corresponding Authors)</h6><div class="year"></div>

	<ul>
        <li>
			<a href="https://arxiv.org/abs/2505.23762" class="publications"> ZeroGUI: Automating Online GUI Learning at Zero Human Cost</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/ZeroGUI">Code</a>]

			</span>
			
			<span class="collaborators">
				Chenyu Yang<sup>*</sup>, Shiqian Su<sup>*</sup>, Shi Liu<sup>*</sup>, Xuan Dong<sup>*</sup>, Yue Yu<sup>*</sup>,<strong>Weijie Su<sup>*</sup><sup>†</sup><sup><i class="far fa-envelope"></i></sup></strong>, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu<sup><i class="far fa-envelope"></i></sup>, Jifeng Dai
			</span>
			
			<span class="collaborators">
				Arxiv Tech Report, 2025.
			</span>
			
		</li>
		<li>
			<a href="https://arxiv.org/abs/2504.10479" class="publications"> InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/InternVL">Code</a>]

				[<a href="https://internvl.github.io/blog/2025-04-11-InternVL-3.0/">Blog</a>]
				
			</span>
			
			<span class="collaborators">
				Jinguo Zhu<sup>*</sup>, Weiyun Wang<sup>*</sup>, Zhe Chen<sup>*</sup>, Zhaoyang Liu<sup>*</sup>, Shenglong Ye<sup>*</sup>, Lixin Gu<sup>*</sup>, Hao Tian<sup>*</sup>, Yuchen Duan<sup>*</sup>, <strong>Weijie Su</strong>, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>, Wenhai Wang<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				Arxiv Tech Report, 2025.
			</span>
			
		</li>
		<li>
			<a href="https://arxiv.org/abs/2412.09613" class="publications"> PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/PVC">Code</a>]
				
			</span>
			
			<span class="collaborators">
				Chenyu Yang<sup>*</sup>, Xuan Dong<sup>*</sup>, Xizhou Zhu<sup>*</sup>, <strong>Weijie Su</strong><sup>*</sup>, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025.
			</span>
			
		</li>
		
		<li>
			<a href="https://arxiv.org/abs/2406.07543" class="publications"> Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/LCL">Code</a>]

				[<a href="https://zhuanlan.zhihu.com/p/703084363?utm_psn=1791107702786764800">Blog</a>]
				
			</span>
			
			<span class="collaborators">
				Chenyu Yang<sup>*</sup>, Xizhou Zhu<sup>*</sup>, Jinguo Zhu<sup>*</sup>, <strong>Weijie Su</strong>, Junjie Wang, Xuan Dong, Wenhai Wang, Lewei Lu, Bin Li, Jie Zhou, Yu Qiao, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				Neural Information Processing Systems (NeurIPS), 2024.
			</span>
			
		</li>
		
                <li>
			<a href="https://arxiv.org/abs/2312.14238" class="publications"> InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/InternVL">Code</a>]
				
				[<a href="https://mp.weixin.qq.com/s/bdfAJRqOF9tUk8Vy9KC_XQ">Blog</a>]
																
			</span>
			
			<span class="collaborators">
				Zhe Chen, Jiannan Wu, Wenhai Wang, <strong>Weijie Su</strong>, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. (<strong>Oral</strong>)
			</span>
			
		</li>
		
		<li>
			<a href="https://arxiv.org/abs/2305.17144" class="publications"> Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/GITM">Project Page</a>]
								
				[<a href="https://medium.com/p/994590ed6760">Blog</a>]
				
				[<a href="https://youtube.com/playlist?list=PL6lJnoxTvWMYKa16ETZtNWaZW5233Noue">Demo</a>]
				
			</span>
			
			<span class="collaborators">
				Xizhou Zhu<sup>*</sup>, Yuntao Chen<sup>*</sup>, Hao Tian<sup>*</sup>, Chenxin Tao<sup>*</sup>, <strong>Weijie Su</strong><sup>*</sup>, Chenyu Yang<sup>*</sup>, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				Arxiv Tech Report, 2023. 
			</span>
			
		</li>
		
		<li>
			
			<a href="https://arxiv.org/abs/2211.09807" class="publications"> Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information</a>
			
			<span class="code_blog">
				
				[<a href="https://github.com/OpenGVLab/M3I-Pretraining">Code</a>]
				
				[<a href="https://www.weijiesu.com/research/M3I-Pretraining/M3I_CVPR_present_without_recording.pdf">Slides</a>]
				
				[<a href="https://www.youtube.com/watch?v=3O_SPwAJ86Y">Presentation</a>]
				
			</span>
			
			<span class="collaborators">
				<strong>Weijie Su</strong><sup>*</sup>, Xizhou Zhu<sup>*</sup><sup><i class="far fa-envelope"></i></sup>, Chenxin Tao<sup>*</sup>, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, Jifeng Dai
			</span>
			
			<span class="collaborators">
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 
			</span>
			
		</li>
		
		<li>
			
			<a href="https://arxiv.org/abs/2206.01204" class="publications"> Siamese Image Modeling for Self-Supervised Vision Representation Learning</a>
			
			<span class="collaborators">
				Chenxin Tao<sup>*</sup>, Xizhou Zhu<sup>*</sup>, <strong>Weijie Su</strong><sup>*</sup>, Gao Huang, Bin Li, Jie Zhou, Yu Qiao, Xiaogang Wang, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.
			</span>
			
		</li>
		
		<li>
			
			<a href="https://arxiv.org/abs/2010.04159" class="publications"> Deformable DETR: Deformable Transformers for End-to-End Object Detection</a>
			

			
			<span class="code_blog">
				
				[<a href="https://github.com/fundamentalvision/Deformable-DETR">Code</a>]
				
				[<a href="https://www.jiqizhixin.com/articles/2020-10-13-6">Blog</a>]
				
				[<a href="https://www.weijiesu.com/research/Deformable-DETR/deformable_detr_iclr_presentation.pdf">Slides</a>]

				[<a href="https://slideslive.com/38953665/deformable-detr-deformable-transformers-for-endtoend-object-detection?ref=speaker-29434-latest">Presentation</a>]
				
			</span>
			

			<span class="collaborators">
				Xizhou Zhu<sup>*</sup>, <strong>Weijie Su</strong><sup>*</sup>, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				International Conference on Learning Representations (ICLR), 2021. (<strong>Oral</strong>)
			</span>
			
		</li>
		
		<li>
			
			<a href="https://arxiv.org/abs/1908.08530" class="publications"> VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a>
			

			
			<span class="code_blog">
				
				[<a href="https://github.com/jackroos/VL-BERT">Code</a>]
				

				

				
				[<a href="https://www.weijiesu.com/research/VL-BERT/VL-BERT-ICLR-present-final.pdf">Slides</a>]
				

				
				[<a href="https://iclr.cc/virtual_2020/poster_SygXPaEYvH.html">Presentation</a>]
				
			</span>
			

			<span class="collaborators">
				<strong>Weijie Su</strong><sup>*</sup>, Xizhou Zhu<sup>*</sup>, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai<sup><i class="far fa-envelope"></i></sup>
			</span>
			
			<span class="collaborators">
				International Conference on Learning Representations (ICLR), 2020.
			</span>
			
		</li>
		
	</ul></div>

		</div>
		</div>

		


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-146428647-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>
	feather.replace()

</script>
		
	</body>

</html>
